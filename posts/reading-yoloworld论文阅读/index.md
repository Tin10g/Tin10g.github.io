# YOLO-World:Real-Time Open-Vocabulary Object Detection


# Abstract
## 提出问题
* 由于预定义和训练过的对象一般是图像类，YOLO系列在开放场景适用性低
## 成果
* 团队通过在较大数据集的视觉语言建模和预训练增强了YOLO开放词汇表检测功能
* RepVL-PAN &amp; region-text contrasitive loss
增强了视觉和语言信息的交互
* 在没有直接训练样本的情况下，模型能高效地识别或检测较大范围内的新的对象类别
## 数据集
* 使用LVIS dataset
* 结果：在LVIS数据集上取得了35.4的平均精度（AP），同时在V100硬件上达到了52.0帧每秒（FPS）
* 对比：在准确性和处理速度上都优于许多当前最先进的方法

# Introduction
* 主流的视觉-语言检测模型和YOLO-World对比，YOLO-World在FPS（v100）上提速20倍，并且在平均精度上和主流模型差不多甚至更好。
&gt; 数据评估方法：1. 精度——LVIS minival的固定的AP 2. 推理速度——NVIDIA V100 w/o TensorRT
* 基于蒸馏方法的局限性：训练数据的稀缺性和又献策i会的多样性
* 一般的区域级视觉-语言比你高大规模训练开放词汇目标检测器的局限性
  * 计算负担大
  * 边缘设备部署复杂
* YOLOWorld主要结构还是YOLO的结构，但是加入了与训练的CLIP文本编译器区编译输入的文本
* 提出RepVL-PAN，把文本特征和图像特征结合，得到更好的视觉-语义表现
* 预训练后的YOLOWorld具有丰富的区域-文本组，它具有更好的对开放词表检测能力，并且训练的数据越多，开放此表能力提升越大
* 提出一种提示后检测范式

# Related Work
## 传统目标检测
* 传统的三种目标检测
  1. 基于区域的方法（Faster-RCNN，RoI-wise）
  2. 基于像素的方法（一阶段检测YOLOs--推理速度快）
  3. 基于查询的方法（起源于DETR）
## 开放词表目标检测
* 在基类上训练检测器并评估新的(未知的)类来达到标准的OVD设置
* 但是这些方法都有使用较大的解码器

# Method
## 预训练公式:区域-文本对
* 相当于把原本传统目标检测的标签（固定值），替换为输入文本（区域对应的文本）。
&gt; 文本可以时类名，名词短语或对象描述
## 模型架构
* 整体结构
  * YOLO检测器
  * 文本编码器
  * RepVL-PAN
* YOLO-World基于YOLOv8结构开发
  * 包含一个作为图片编码器的Darknet backbone【多尺度特征金字塔的路径聚合网络】
  * 一个用于边界框回归和对象嵌入的heading
* 文本编码器
  * CLIP作文本编码器
* 文本对比头
  * 依旧采用解耦头和两个3x3卷积来回归边界框和对象嵌入
  * 用来获得对象-文本相似度
  * 矩阵乘法
* 在线词汇训练
  * 每个包含4张图像的马赛克样本构建一个在线词汇T
  * 所有肯定名词进行采样，并从相应的数据集中随机抽取一些否定名词
* 离线词汇训练
  * 推理阶段，使用离线词汇表提示-检测策略，提高效率。用户可以自定义提示
  * 利用文本编码器对这些提示进行编码，并获得离线词汇嵌入
  * 离线词汇嵌入可以重新参数化为卷积层或线性层的权重，以便部署
* 文本引导的交叉阶段部分层（Text-guided CSPLayer）
* 图像池化注意力（Image-Pooling Attention）
## 可重参数化的视觉-语言路径聚合网络
* 自上而下和自下而上的路径
* 通过多尺度图像特征{C3, C4, C5}，建立特征金字塔{P3, P4, P5}
## 预训练方案
* 从区域-文本对比损失中学习
  * 通过区域-文本对比损失（Lcon），利用摩赛克样本和文本，对模型输出的目标预测与真实标注进行匹配，构建损失函数，其中包含IoU损失和分布式焦点损失
* 图像-文本数据的伪标记
  * 提出了一个自动标注方法
  * 三个步骤
    1. 提取名词短语 
    2. 使用预训练的开放词汇检测器生成伪框
    3. 利用CLIP评估和过滤低相关的图像-文本和区域-文本对
  * 该方法从CC3M数据集中提取并标注了246,000张图像，生成821,000个伪标注，构建了CC3M-Lite数据集

# Experiments
* 效能展示：在大规模数据集上预训练过的YOLO-World
* 评估方法：在LVIS基准和COCO基准上的一个零样本方法
* 同时评估微调的YOLO-World在COCO和LVIS目标检测效果
## 实现细节
* 开发基础：MMYOLO工具箱、MMDetection工具箱
&gt; **MMYOLO toolbox** 一种基于MMDetection**框架**的目标检测工具箱，提供高效的目标模型训练和推理功能。支持多种目标检测算法和一系列功能：模型配置、数据集支持、搞笑训练和推理、丰富工具和功能。
* 依据不同延迟需求，提供了三种YOLO-World变体【S、M、L】
* NVIDIA V100 GPU上测量所有模型的推理速度，而不需要额外的加速机制
## 预训练
1. 实验设置
   * AdamW优化器
   * 初始学习率：0.002
   * 权值衰减：0.05 
   * 初始：在32个NVIDIA V100 gpu上100次预训练，总批次 batch size 512
   * 数据增强：4张图像颜色增强、随机仿射、随机翻转、拼接等（文本编码器在预训练期间冻结）
   &gt; **冻结文本编码器**意味着在训练过程中，模型不会对其进行反向传播和权重更新，从而保持其原有的特征提取能力
2. 预训练数据
   *  预训练模型，主要使用了一些检测和定位的数据集，比如Objects365、GQA和Flickr30k
   *  为了避免数据重复或减少模型的偏差，排除了来自COCO数据集的图像，否则容易出现过拟合。同时确保模型学习到更广泛和多样化的数据特征，提高其在真实场景中的表现
   *  为了让模型在学习时能够获取更多的信息和特征，加了图像-文本对的数据，具体是从CC3M数据集中抽取并标注了246,000张图像，形成了一个新的CC3M-Lite数据集。
3. 零样本评估
   * LIVIS 数据集包含1203个对象类
4. YOLO-World在LVIS目标检测基准上的主要结果
   * 对比试验： YOLO-World与近期一些最先进的方法进行比较，这些方法在类似的数据集上进行了预训练，并采用了较轻的网络骨干（例如Swin-T）
   * YOLO-World在零样本性能和推理速度上均优于之前的方法，尽管它使用的模型参数更少。
   * 尽管GLIP、GLIPv2和Grounding DINO等方法使用了更多的数据（例如Cap4M），YOLO-World在O365和GolG数据集上预训练后，依然取得了更好的性能
   * YOLO-World与DetCLIP的性能相当（35.4对34.4），但推理速度提高了20倍
   * 小模型能力： 实验结果表明，即使是参数较少的模型（如YOLO-World-S，仅有1300万参数），也能够进行视觉-语言预训练，并具备强大的开放词汇能力。
## 消融实验
&gt; 相对广泛
* 预训练数据
  * YOLO-World使用不同的数据集进行预训练，特别是在Objects365的基础上，加入GQA数据集后，在LVIS上的平均精度（AP）提升了8.4。
  &gt; 因为GQA提供了更丰富的文本信息，有助于模型更好地识别大量词汇的对象
  * 进一步添加CC3M样本（仅8%的完整数据集）带来了0.5 AP的提升，尤其在稀有对象上的提升为1.3 AP。表明数据量能够有效提高在大词汇场景下的检测能力。
* RepVL-PAN的消融实验
  * 展示了RepVL-PAN（包括文本引导的CSPLayers和图像池化注意力）对零样本LVIS检测的有效性
  * 比较在O365和O365与GQA共同预训练的结果，RepVL-PAN相较于基线模型YOLOv8-PAN提升了1.1 AP，尤其在难以检测的稀有类别上表现更为显著。
  * 预训练使用GQA数据集时，性能提升更加明显，说明丰富的文本信息对模型的帮助
* 文本编码器的比较
  * 比较了不同的文本编码器（BERT-base和CLIP-base）在预训练中的表现。
  * 实验分为冻结和微调两种设置，微调时学习率为基本学习率的0.01倍
  * CLIP文本编码器的表现优于BERT，在稀有类别上提高了10.1 AP
  * 微调BERT在预训练中显著提升了性能（&#43;3.7 AP），但微调CLIP则导致性能严重下降 =&gt; 因为其类别较少，文本信息也不够丰富
# 微调YOLO-World
* 实验设置
  * 预训练权重进行初始化，并进行了80个周期的细化训练，采用AdamW优化器，初始学习率为0.0002
  * 在LVIS数据集上，按照之前的研究，YOLO-World在LVIS-base（常见和频繁的类别）上进行细化训练，并在LVIS-novel（稀有类别）上进行评估。同时，对文本编码器的学习率设置为0.01
* COCO目标检测
  * 在COCO数据集上，YOLO-World与之前的YOLO检测器进行比较。为加快训练过程，由于COCO数据集的词汇量较小，移除了提出的RepVL-PAN
  * 实验结果显示，YOLO-World在COCO数据集上能够实现相当不错的零样本性能，表明其具有较强的泛化能力。
  * 此外，经过80个周期的细化训练后，YOLO-World在COCO train2017上表现优于之前从头训练且训练周期≥300的其他方法。
* 零样本评估在LVIS上的结果
  * 展示了YOLO-World与其他最新方法在LVIS数据集上的比较，包括模型架构、参数数量、预训练数据、FPS（帧每秒）和AP（平均精度）
  * YOLO-World-S、YOLO-World-M和YOLO-World-L在零样本检测中均表现出色，尤其是YOLO-World-L的AP达到35.4，显示了其在大词汇检测中的有效性
* 预训练数据的消融实验
  * 展示了使用不同数据进行预训练的影响。增加GQA等数据集显著提升了AP，尤其是在稀有对象上的检测能力得到了改善



---

> Author: [Ting](Tin10g.github.io)  
> URL: http://localhost:1313/posts/reading-yoloworld%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/  

